{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLamaIndex + GitHub Models for RAG\n",
    "\n",
    "This notebook demonstrates how to perform Retrieval-Augmented Generation (RAG) with LLamaIndex.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique in natural language processing that combines the strengths of retrieval-based and generation-based methods to enhance the quality and accuracy of generated text. It integrates a retriever module, which searches a large corpus of documents for relevant information, with a generator module to produce coherent and contextually appropriate responses. This hybrid approach allows RAG to leverage vast amounts of external knowledge stored in documents, making it particularly effective for tasks requiring detailed information and context beyond the model's pre-existing knowledge.\n",
    "\n",
    "RAG operates by first using the retriever to identify the most relevant pieces of information from a database or collection of texts. These retrieved passages are then fed into the generator, which synthesizes the information to produce a final response. This process enables the model to provide more accurate and informative answers, as it dynamically incorporates up-to-date and specific details from the retrieval stage. The combination of retrieval and generation ensures that RAG models are both knowledgeable and flexible, making them valuable for applications such as question answering, summarization, and dialogue systems.\n",
    "\n",
    "In this sample, we will create an index from a set of markdown documents that contain product descriptions. Using a retriever, we will search the index with a user question to find the most relevant documents. Then we will use llama-index's query for a full Retrieval-Augmented Generation (RAG) implementation.\n",
    "\n",
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install openai\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup classes to a chat model and an embedding model\n",
    "\n",
    "To run RAG, you need 2 models: a chat model, and an embedding model. The GitHub Model service offers different options.\n",
    "\n",
    "For instance you could use an Azure OpenAI chat model (`gpt-4o-mini`) and embedding model (`text-embedding-3-small`), or a Cohere chat model (`Cohere-command-r-plus`) and embedding model (`Cohere-embed-v3-multilingual`).\n",
    "\n",
    "We'll proceed using some of the Azure OpenAI models below. You can find [how to leverage Cohere models in the LlamaIndex documentation](https://docs.llamaindex.ai/en/stable/examples/llm/cohere/).\n",
    "\n",
    "### Example using Azure OpenAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    raise ValueError(\"GITHUB_TOKEN is not set\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GITHUB_TOKEN\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://models.inference.ai.azure.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are setting up the embedding model and the llm model to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "import logging\n",
    "import sys, os\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    ")\n",
    "\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an index and retriever\n",
    "\n",
    "In the data folder, we have some product information files in markdown format. Here is a sample of the data:\n",
    "\n",
    "```markdown\n",
    "# Information about product item_number: 1\n",
    "TrailMaster X4 Tent, price $250,\n",
    "\n",
    "\n",
    "## Brand\n",
    "OutdoorLiving\n",
    "\n",
    "Main Category: CAMPING & HIKING\n",
    "Sub Category: TENTS & SHELTERS\n",
    "Product Type: BACKPACKING TENTS\n",
    "\n",
    "## Features\n",
    "- Polyester material for durability\n",
    "- Spacious interior to accommodate multiple people\n",
    "- Easy setup with included instructions\n",
    "- Water-resistant construction to withstand light rain\n",
    "- Mesh panels for ventilation and insect protection\n",
    "- Rainfly included for added weather protection\n",
    "- Multiple doors for convenient entry and exit\n",
    "- Interior pockets for organizing small items\n",
    "- Reflective guy lines for improved visibility at night\n",
    "- Freestanding design for easy setup and relocation\n",
    "- Carry bag included for convenient storage and transportation\n",
    "```\n",
    "Here is the link to the full file: [data/product_info_1.md](data/product_info_1.md). As you can see, the files are rather long and contain different sections like Brand, Features, User Guide, Warranty Information, Reviews, etc. All these can be useful when answering user questions.\n",
    "\n",
    "To be able to find the right information, we will create a vector index that stores the embeddings of the documents. Note that we are reducing the batch size of the indexer to prevent rate limiting. The GitHub Model Service is rate limited to 64K tokens per request for embedding models.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: we have to reduce the batch size to stay within the token limits of the free service\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, insert_batch_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an index, we can use the retriever to find the most relevant documents for a user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever()\n",
    "fragments = retriever.retrieve(\"What is the temperature rating of the cozynights sleeping bag?\")\n",
    "\n",
    "for fragment in fragments:\n",
    "    print(fragment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use chat model to generate an answer\n",
    "\n",
    "Now that we have the documents that match the user question, we can ask our chat model to generate an answer based on the retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "context = \"\\n------\\n\".join([ fragment.text for fragment in fragments ])\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant that answers some questions with the help of some context data.\\n\\nHere is the context data:\\n\\n\" + context),\n",
    "    ChatMessage(role=\"user\", content=\"What is the temperature rating of the cozynights sleeping bag?\")\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLamaIndex provides a simple API to query the retriever and the generator in one go. The query function takes the question as input and returns the answer generated by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the temperature rating of the cozynights sleeping bag?\")\n",
    "print()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is a good 2 person tent?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Does the SkyView 2-Person Tent have a rain fly?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gh-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
